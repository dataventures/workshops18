{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A light introduction to natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lab 3, we're going to briefly cover some methods for analyzing data that comes in the form of text. This will help with the practical next week "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification in Movie Reviews\n",
    "\n",
    "We'll use the dataset from Stanford here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "1) First click on the link and download the dataset (It's too big to put on github)\n",
    "\n",
    "2) Make sure you move the directory \"aclImdb\" into the same folder as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the data comes in separate files, which is kind of annoying. I used glob for this. glob(\"directory/*\") just lists the filenames in that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#glob lets us quickly access all the filenames, either pip install it or find a different way to do this\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_filenames = glob('aclImdb/train/pos/*')\n",
    "neg_filenames = glob('aclImdb/train/neg/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check now that pos_filenames has all the filenames for positive reviews and neg_filenames has all the filenames for negative reviews. The following code is pretty hacky, but it does the job for combining all the text into one dataframe. We'll just open the files one by one in a list and append each string to a list. We'll also keep track of the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loop through the list of files and append the contents to a list\n",
    "contents = []\n",
    "sentiments = []\n",
    "\n",
    "#loop through the positive sentiment files and save all the contents\n",
    "for fname in pos_filenames:\n",
    "    with open(fname,'rb') as f:\n",
    "        contents.append(str(f.readlines()[0]))\n",
    "        sentiments.append(1)\n",
    "        \n",
    "for fname in neg_filenames:\n",
    "    with open(fname,'rb') as f:\n",
    "        contents.append(str(f.readlines()[0]))\n",
    "        sentiments.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the length of the list we just made (total number of movie revieews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first movie review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'\n"
     ]
    }
   ],
   "source": [
    "print(contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get back to familiar territory, we'll turn this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can turn this into a pd Dataframe\n",
    "df = pd.DataFrame()\n",
    "df['txt'] = contents\n",
    "df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  sentiment\n",
       "0  b'Bromwell High is a cartoon comedy. It ran at...          1\n",
       "1  b'Homelessness (or Houselessness as George Car...          1\n",
       "2  b'Brilliant over-acting by Lesley Ann Warren. ...          1\n",
       "3  b'This is easily the most underrated film inn ...          1\n",
       "4  b'This is not the typical Mel Brooks film. It ...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool. But we still don't really know how to deal with this. Computers aren't inherently able to understand text, so we'll need to get the \"txt\" column into a form we know how to work with in order to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn\n",
    "\n",
    "sklearn isn't really the best library for working with text data, so we'll keep this section relatively short. For most purposes you'll want to use NLTK or spacy. But since you're familiar with sklearn we'll start here. \n",
    "\n",
    "The main thing that we'll be using from sklearn is CountVectorizer. This will take a corpus of text and turn each document into a \"count vector.\" This count vector is essentially a histogram over the entire vocabulary (all words in the training set). As an example, consider the (fake) sentence \"dog cat cat cat bear\". Our vocab size is 3, so the sentence is represented by the three dimensional vector:\n",
    "\n",
    "$$[1,3,1] $$\n",
    "\n",
    "This is also called the bag of words representation. \n",
    "\n",
    "**Exercise 1:** what are the pros and cons of using this? Can you think of an alternative way of representing text at the sentence level? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a built in count vectorizer\n",
    "- Fit: build vocabulary on some iterable containing strings\n",
    "- transform: use existing vocabulary to transform the input into a N x V sparse matrix\n",
    "- fit_transform: fit on this data, and also transform it (same as calling fit then transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#by default countvectorizer will return a sparse array which is a special datatype for \n",
    "#arrays that are mostly 0s (to save space), but .toarray() will convert this back to a\n",
    "#regular numpy array\n",
    "cv.fit_transform([\"cat dog dog dog cow\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example consider the sentence \"dog dog dog snail snail\". Since \"snail\" is not in the original vocab that we fit count_vectorizer with, it won't be part of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"dog dog dog snail snail\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Use CountVectorizer on the Stanford dataset. This will take a few seconds. You'll want to use max_features to limit the number of words that you consider, since rare words won't help you much.  \n",
    "\n",
    "Limit the number of features to 10,000\n",
    "\n",
    "Save the result as new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution:\n",
    "\n",
    "new_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams vs Bigrams vs N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explored above, using this \"Bag of Words\" representation throws away a lot of information in the sentence. One way of trying to preserve local information is to use bigrams. This is just expanding our vocabulary to include consecutive word-pairs of length 2. So in our example: \"cat dog dog dog cow\", we would have the vocabulary\n",
    "- cat x1 \n",
    "- dog x3\n",
    "- cow x1\n",
    "- cat dog x1\n",
    "- dog dog x2 \n",
    "- dog cow x1\n",
    "\n",
    "N-grams is the extension of this to word sequences of length N. For CountVectorizer, we specify this with:\n",
    "\n",
    "ngram_range = (1, N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 3, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "cv.fit_transform([\"cat dog dog dog cow\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple baseline model\n",
    "\n",
    "For text classification, simple models trained on a large amount of data perform quite well. A pretty standard baseline is the Naive Bayes Model. We'll go through some of the math here. If you're not interested in it, you can skip it.\n",
    "\n",
    "Suppose that $y_i$ is your class label (in this case $y_i$ is either 0 or 1 for negative and positive). $i$ just indexes what datapoint you're looking at. We'll say that $i$ ranges from $1$ to $N$ (in other words you have $N$ sentences in your training dataset).\n",
    "\n",
    "Also we have $\\mathbf{x}_i$ which is the sentence corresponding to the label $y_i$. We use bold to denote the fact that $\\mathbf{x}_i$ is a vector where each element is a word.\n",
    "\n",
    "Naive Bayes models the joint probability density, $p(y_i, \\mathbf{x}_i)$. We do this by parameterizing the prior probability of having a certain class, and then parameterizing the probability of generating a certain sentence given that class. Using Bayes rule, we can write this as:\n",
    "\n",
    "$$p(y_i, \\mathbf{x}_i) = p(\\mathbf{x}_i | y_i) p(y_i) = p(x_{i1},\\dots,x_{iT_i} | y_i) p(y_i) $$\n",
    "\n",
    "where $x_{it}$ is the word at position $t$, and $T_i$ is the length of sentence $i$. Now we apply a huge assumption (which seems like it is just wrong, but works decently in practice). That is, we assume that $x_{i1},\\dots,x_{iT_i}$ are conditionally independent given the class, $y_i$. This lets us factor the probability as:\n",
    "\n",
    "$$p(y_i, \\mathbf{x}_i) = p(y_i)\\prod_{t=1}^T p(x_{it}| y_i)  $$ \n",
    "\n",
    "We parameterize $p(x_{it}|y_i)$ as a Multinoulli random variable, i.e.:\n",
    "\n",
    "$$p(x_{it} = dog | y_i = 0) =  \\pi_{0,dog} $$ \n",
    "\n",
    "Where $\\pi_{0,dog}$ is the probability that \"dog\" is generated given that we're in class 0 (negative). So we need 2*Vocab_size parameters for this, since we need a probability for every class for every word. For the english language, that's approximately 20,000 parameters. Also, we parameterize the prior probabilites as bernoulli random variables. That's only one extra parameter:\n",
    "\n",
    "$$p(y_i = 0) = \\theta_0$$\n",
    "$$p(y_i = 1) = 1 - \\theta_0$$\n",
    "\n",
    "Given this model, it's pretty straightforward to get a maximum likelihood estimate for all the parameters, i.e. the $\\theta$s and $\\pi$s. If you're not familiar with maximum likelihood, it just means that we choose $\\theta$ and $\\pi$s to be the values that make the observed data have the highest likelihood. This turns the learning procedure into a simple optimization problem. \n",
    "\n",
    "If we go through all the math to solve this, it actually turns out that we get:\n",
    "optimal $\\theta_0$ is the proportion of sentences that are in class $0$, the negative class, and that $\\pi_{0,dog}$ is just the proportion of words in class $0$ that are the word \"dog\". Likewise, $\\pi_{1,dog}$ is just the proportion of words in class $1$ that are the word \"dog\". \n",
    "\n",
    "So \"training\" the model is just learning these parameters through an optimization procedure. But given a sentence, how do we make a prediction of whether its positive of negative?\n",
    "\n",
    "We can express that as:\n",
    "\n",
    "$$p(y_i = 0 | \\mathbf{x}_i ) \\propto p(\\mathbf{x}_i|y_i = 0) p(y_i = 0) $$\n",
    "$$p(y_i = 1 | \\mathbf{x}_i ) \\propto p(\\mathbf{x}_i|y_i = 1) p(y_i = 1) $$\n",
    "\n",
    "To predict we just take the higher of these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional (hard) exercise:** implement Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, this is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the model with the normal sklearn syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-76aabacd845a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_data' is not defined"
     ]
    }
   ],
   "source": [
    "nb.fit(new_data, df['sentiment']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code splits into train and test for evaluation of the model. We'll make a random permutation of indices and then use that to randomly shuffle our data. We'll take a 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.permutation makes a random permutation of indices {1...N} \n",
    "perm = np.random.permutation(range(len(df.sentiment)))\n",
    "\n",
    "#split this permutation by a 70:30 split\n",
    "trn = perm[:int(0.7*len(perm))]\n",
    "tst = perm[int(0.7*len(perm)):]\n",
    "\n",
    "#slice the processed data into train and test sets; alternatively we could have done this with \n",
    "#sklearn functions\n",
    "x_train = new_data[trn]\n",
    "x_tst = new_data[tst]\n",
    "y_train = df.sentiment[trn]\n",
    "y_tst = df.sentiment[tst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape of our splits. The test set has 7500 data points and the trianing set has 17500. We have 10,000 features since there are 10,000 words in our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Naive Bayes model on our training x and y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score will give us our classification accuracy on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(x_tst,y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good for such a simple model. Obviously, we'll do a bit better on the data that we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Improve the score somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) building a more powerful model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using torchtext (preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to save out tabular dataset from above as a csv and then load it with torchtext because I couldn't think of a better way to do this off the top of my head. Torchtext is a library for loading/dealing with datasets for pytorch. And pytorch is a library for implementing neural networks (similar to tensorflow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "\n",
    "df.iloc[trn,:].to_csv('saved_dataset_train.csv',index = False,header = False)\n",
    "df.iloc[tst,:].to_csv('saved_dataset_test.csv',index = False,header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by initializing two torchtext Field objects, which will hold label and text vocabularies. We'll load in the two datasets using these fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field()\n",
    "LABEL = torchtext.data.Field(sequential = False)\n",
    "pos_train = torchtext.data.TabularDataset(path='saved_dataset_train.csv', format='csv',fields=[('txt', TEXT),\n",
    " ('sentiment', LABEL)])\n",
    "\n",
    "pos_test = torchtext.data.TabularDataset(path='saved_dataset_test.csv', format='csv',fields=[('txt', TEXT),\n",
    " ('sentiment', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary using the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 235807\n",
      "len(LABEL.vocab) 3\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(pos_train)\n",
    "LABEL.build_vocab(pos_train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (pos_train,pos_test), batch_size=10, device=-1,sort_key=lambda x: len(x.txt),repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.batch.Batch at 0x15b3c9ef0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make use of pretrained Word Embeddings by Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([235807, 300])\n",
      "Word embedding of 'follows', first 10 dim  \n",
      " 0.3925\n",
      "-0.4770\n",
      " 0.1754\n",
      "-0.0845\n",
      " 0.1396\n",
      " 0.3722\n",
      "-0.0878\n",
      "-0.2398\n",
      " 0.0367\n",
      " 0.2800\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay cool. Now that all the preprocessing stuff is done, we can focus on actually building a model. We're going to build a convolutional neural network in pytorch. This involves building a CNN class that inherits nn.Module. We'll implement this paper by Yoon Kim: http://aclweb.org/anthology/D/D14/D14-1181.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class customConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_embeddings, embedding_dim = 300, hidden_size = 100, vocab_size = 235807):\n",
    "        super(customConvNet,self).__init__()  \n",
    "        embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        embedding.weight = nn.Parameter(input_embeddings)\n",
    "        self.embedding = embedding\n",
    "        self.conv3 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 3,stride = 1)\n",
    "        self.conv4 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 4,stride = 1)\n",
    "        self.conv5 = nn.Conv1d(embedding_dim,hidden_size,kernel_size = 5,stride = 1)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear = nn.Linear(3*hidden_size,3)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        #apply embedding layer\n",
    "        embeds = self.embedding(input_).permute(1,2,0).contiguous()\n",
    "        #apply convolution layers\n",
    "        out1 = self.conv3(embeds)\n",
    "        out2 = self.conv4(embeds)\n",
    "        out3 = self.conv5(embeds)\n",
    "        \n",
    "        #apply max pooling layers\n",
    "        out1 = self.maxpool(out1).squeeze(2)\n",
    "        out2 = self.maxpool(out2).squeeze(2)\n",
    "        out3 = self.maxpool(out3).squeeze(2)\n",
    "        #concatenate the outputs; ending up with a batch_size x 3*hidden_size vector\n",
    "        out = torch.cat((out1,out2,out3),dim = 1)\n",
    "        out = self.dropout(out)\n",
    "        return self.linear(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize an instance of the neural network, and make sure it produces output when we feed in a batch of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = customConvNet(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.2616 -0.0998 -0.0296\n",
       "-0.2551 -0.4493 -0.1118\n",
       "-0.1072 -0.2760 -0.0689\n",
       "-0.1006 -0.1326  0.0534\n",
       "-0.1832 -0.1641  0.0457\n",
       "-0.0593 -0.0350 -0.1146\n",
       "-0.0945 -0.3983 -0.0943\n",
       " 0.1247  0.1473 -0.1453\n",
       "-0.1965 -0.1806 -0.0194\n",
       " 0.1082 -0.2329 -0.0160\n",
       "[torch.FloatTensor of size 10x3]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn(batch.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that looks reasonable. The next thing we have to do is write a training loop that will optimize the Convolutional Neural Networks parameters using Stochastic Gradient Descent.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def model_train(model,train_iter,num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(num_epochs),desc = 'Epoch'):\n",
    "        total_loss = 0 \n",
    "        count = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm_notebook(train_iter, desc = 'batch'):\n",
    "            optimizer.zero_grad()\n",
    "            txt = batch.txt\n",
    "            lbl = batch.sentiment\n",
    "            \n",
    "            loss = criterion(model(txt),lbl)\n",
    "            total_loss += loss.data\n",
    "            count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if count % 50 == 1:\n",
    "                print(\"Average NLL: \", (total_loss/count)) \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a71a57055b4502b80d129cc4cae30c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccd1dce17014236ac06c444888a1fd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  \n",
      " 0.7661\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/anaconda/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  \n",
      " 0.7285\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7158\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7130\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7111\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7071\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7059\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7023\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.7003\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6985\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6972\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6970\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6960\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6954\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6943\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6931\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6911\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6903\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6876\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6858\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6820\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6795\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6778\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6768\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6746\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6733\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6703\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6654\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6636\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6618\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6603\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6580\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6554\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6529\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6510\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6487\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6463\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6420\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6400\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6385\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6366\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6348\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6312\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6288\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6273\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6259\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6247\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6226\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6190\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6177\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6163\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6147\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6135\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6116\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6101\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6070\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6060\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6050\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6036\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6020\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.6004\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5975\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5960\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5941\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5926\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5906\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5890\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5878\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5861\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5848\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5831\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5816\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5801\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5784\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5769\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5755\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5744\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5728\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5714\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5703\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5690\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5680\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5667\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5652\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5647\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5639\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5624\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5611\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5600\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5575\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5568\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5558\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5545\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5535\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5524\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5514\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5478\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5465\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5429\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5415\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5403\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5387\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5376\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5360\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5349\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5336\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5325\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5313\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5304\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5286\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5277\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5268\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5258\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5247\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5236\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5226\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5217\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5210\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5203\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5194\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5188\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5180\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5170\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5162\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5154\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5146\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5135\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5126\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5117\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5109\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5098\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average NLL:  \n",
      " 0.5089\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-36dcd1e67dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-275-4087db6c93f3>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, train_iter, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_train(cn,train_iter,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
