{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Author: Russell Kunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of regression models\n",
    "\n",
    "1. Linear Regression\n",
    "2. Decision Trees / Random Forests\n",
    "3. Logistic Regression\n",
    "4. More complex models: SVM and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Linear regression is a staple in statistical inference, and though not flexible enough to have very much predictive power, it serves as the basis for many more complex models. Many of you are probably already familiar with this concept, so we'll try to do a bit better. \n",
    "\n",
    "The general setup that we'll face for regression problems is that we'll have $n$ sets of variables, including a 1 dimensional response variable $y$, and a p-dimensional feature vector $\\mathbf{x}$. You can think of the response as the thing you're trying to predict, and the feature as the $p$ quantities that you're using as input. We can write this as follows:\n",
    "\n",
    "$$(y_1,\\mathbf{x_1}),\\dots,(y_n,\\mathbf{x_n})  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
