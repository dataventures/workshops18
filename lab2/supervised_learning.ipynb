{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing useful python packages\n",
    "\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone over the data science pipeline, we will start looking at different ways to solve problems with data. Supervised learning is a branch of machine learning focused on making predictions given a set of data. \n",
    "\n",
    "In supervised learning, the model defines the effect one set of observations, called inputs, has on another set of observations, called outputs. We are given a dataset with response variables and predictor variables and use this to try to come up with a function that maps predictor variables to response variables. We assume that there is some true mapping function $f$, and try to come up with an approximation $\\hat f$ so that given new data, we can make accurate predictions.\n",
    "\n",
    "For example, consider the process of predicting the price of a house from its features (sq. feet, location, number of bedrooms, etc). We assume that there exists some relationship between housing price and these features. In order to find this relationship, we look at data where house price and features are both given, and we try to fit a model mapping features to housing price that we think best describes hour training data. We can then apply this model to test data - that is, data where we have features but don't know the actual price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical supervised learning problem, we have some number of predictors variables and a response (or outcome) variable. The outcome variable is the variable that we would like to predict. The predictor variables are the ones that we have at our disposal to try to determine the outcome variable. We usually label the number of predictor variables as $p$, and we call the $i$th instance of the $j$th predictor variable $X_{i,j}$. We call the $i$th instance of the response variable $Y_i$. Generally we write $Y = f(X) + \\epsilon$, where $\\epsilon$ represents the true error of the relationship between $X$ and $Y$, which is impossible to predict. \n",
    "\n",
    "Summary of Notation:\n",
    "\n",
    "$\\hat{Y_i}$ is the predicted outcome from our model. \n",
    "\n",
    "$Y_i$ is the true outcome. \n",
    "\n",
    "$X_{i,j}$ is the jth predictor variable\n",
    "\n",
    "We sometimes refer to the predictor variables collectively as a vector: $X_i$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of supervised learning problems: regression and classification. In regression, we have a continuous response variable with ordering. In classification, we have discrete response variables, without ordering. \n",
    "\n",
    "The housing price problem is an example of a regression problem - housing prices are continuous, with a logical ordering.\n",
    "\n",
    "In a classification problem we might try to predict whether someone has a disease or doesn't have a disease based on various health metrics. Here we are classifying inputs into 2 categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance: Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip past the actual model fitting for now (which is unique to each type of model), taking us to the general process of model evaluation. Once we have a model, to evaluate how good it is we need some metric of performance. We call this metric the loss function. Below are two typical examples of loss functions, used for regression problems.\n",
    "\n",
    "Residual Sum of Squares: $RSS = \\sum_{i=1}^n{(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "Mean Squared Error: $MSE = \\frac{1}{n} \\sum_{i=1}^n{(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "Intuitively, these loss functions capture how far off the predicted values are from the actual values. Mean squared error is a normalization of RSS - for example, an accurate model tested on a huge dataset may have a larger RSS than an inaccurate model tested on a small dataset, and MSE simply normalizes for size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Test Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to validate our model is to split the dataset we're given into random subsets, training and test. This is to prevent overfitting, where our predictive model is taking into account fluctuations due to the irreducible error ($\\epsilon$) in the dataset rather than solely the function $f$ that relates $X$ (predictors) and $Y$ (response). Because of this, it's important to validate our model by testing it on a dataset other than the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "K Nearest Neighbors is one of the most intuitive supervised learning methods. It is centered around the idea that similar inputs should have similar labels. As a concrete example, KNN works off the assumption that houses with similar locations and similar numbers of bedrooms have similar prices - as a result, when we see a house that we want to predict the price of, we look for similar houses in the neighborhood and average their prices. The kNN algorithm performs exactly this process.\n",
    "\n",
    "Given an input i that we want to predict an output for, we find the k inputs in the input space most similar to i. Then, average the outputs of those k inputs to predict the output for i. Unlike regression, there is no training phase, since we are not trying to find a function between the input and output spaces. \n",
    "\n",
    "The steps for kNN are:\n",
    "\n",
    "1. Take in a new instance that we want to predict the output for\n",
    "\n",
    "2. Iterate through the dataset, compiling a set S of the k closest inputs to i\n",
    "\n",
    "3. Calculate the nearest neighbors response by looking at the corresponding outputs to the inputs in S - average the outputs for regression and take the majority vote for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Decisions\n",
    "\n",
    "In the basic kNN model, the only decisions we need to make are what to choose for the value of k and how to define the distance between inputs in order to determine our definition of similarity. \n",
    "\n",
    "For large values of k we get less distinction between classes, as we are then just averaging over large subsets of the entire dataset. On the other hand, for small values of k, our predictions may be strongly affected by noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "kNN is a very easy algorithm to understand. However, there are several major drawbacks. First off, notice that KNN does not find an explicit function mapping inputs to outputs, like regression does. Instead, it searches through the dataset to find the neighbors. This forces us to store the entire training dataset. In addition, the process of traversing the dataset can be very expensive for large datasets. Finally, as we saw earlier, choosing the correct value of k is crucial to getting a high performing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pain to have to implement this algorithm every time you want to use the kNN method for prediction. Scikit-learn is a Python library that implements just about every machine learning model you will need for these workshops. The goal of these workshops is to teach you the theory behind machine learning methods (which will help you decide what methods are appropriate for what situations), while helping you apply these methods to real world problems. For the latter purpose, Scikit-learn will be incredibly useful. As always, the [library documentation](http://scikit-learn.org/stable/) is the best resource for learning how to use the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3], [4]]\n",
    "y = [0, 0, 1, 1, 2]\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=3)\n",
    "neigh.fit(X, y) \n",
    "print(neigh.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning regression, which you may have seen before, is a method for modeling continuous data. We're teaching it because it's used a *lot* in practice, and it's fairly simple to get your head around.\n",
    "\n",
    "## Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "Setting the problem up, we are given a set of training pairs $(x_i, y_i)$, and we want to find $m$ and $b$ to maximize the function $f(x) = mx + b$. Most supervised learning problems will be of this form - we assume some sort of model mapping inputs to output (in this case it's just a linear function), and we want to tune the parameters (in this case $m$ and $b$) that give the model the best fit on the training pairs. How should we find these optimal values?\n",
    "\n",
    "Intuitively, we want $m$ and $b$ that minimize the *error* between predicted values and actual values - that is, the difference between $f(x_i)$ and $y_i$. We sum the squared error $(f(x_i) - y_i)^2$ over all $i$ in order to get a metric for how well our model fits on the data. Note that we use squared error since we want error to measure the absolute distance between the predicted value and the actual value. \n",
    "\n",
    "The goal with OLS is to minimize this sum of squares error, known as the L2 error. Consider a candidate $m$ and $b$. The sum of squares error is \n",
    "$$\\sum_{i = 1}^n (mx_i + b - y_i)^2$$\n",
    "To minimize this, we can take the derivative w.r.t. $m$ and the derivative w.r.t. $b$, and set them equal to 0. We get:\n",
    "\n",
    "$$\\sum_{i = 1}^n 2(mx_i + b - y_i) = 0$$\n",
    "\n",
    "$$\\sum_{i = 1}^n 2x_i(mx_i + b - y_i) = 0$$\n",
    "\n",
    "Remember that we know every value except for $m$ and $b$. This gives us, then, a system of 2 equations in 2 variables. Solving that system gives us the OLS solution:\n",
    "\n",
    "$$m = \\frac{\\overline{xy} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2}$$\n",
    "$$b = \\overline{y} - m\\cdot\\overline{x}$$\n",
    "\n",
    "where we've used the notation $\\overline{x}$ to mean the average of the $x_i$. Note in particular that the average $\\overline{xy}$ means $(\\sum_{i = 1}^n x_iy_i)/N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Much Theory Should I Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind this single variable linear model is pretty straightforward, but future models can get pretty complicated in terms of math. Don't worry if you cannot understand the entire theory behind every model - often, knowing the intuition behind the model is enough, but we encourage you to try to learn as much of the math as you can in order to get comfortable with the model. \n",
    "\n",
    "We also encourage you to pay special attention to the processes that are common to all supervised learning models. For this example, we \n",
    "\n",
    "1. Assumed a linear model between $x$ and $y$ (giving us the parameters $m$ and $b$ to optimize over)\n",
    "\n",
    "2. Define a loss function (which maps each combination of $m$ and $b$ to their error) to measure how well a set of parameters fits on the data\n",
    "\n",
    "3. Minimize the loss function over the parameters. This is where the complicated math may come in\n",
    "\n",
    "You will see this process repeated several times for different types of models, so make sure you understand what the general form of fitting a machine learning model entails!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See this in Practice\n",
    "\n",
    "First, we gennerate a data set over the points $x \\in [1, 100]$ by adding noise to $y$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(100)/100.\n",
    "# generate y using a 'true' mean and y intercept, and then add noise\n",
    "y = (x*2.02 + 5.) + (np.random.rand(100) * 0.1 - 0.05)\n",
    "plt.plot(x, y, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15)\n",
    "\n",
    "# unfortunately, the shape of x_train and x_test isn't quite right\n",
    "x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1)\n",
    "\n",
    "# let's use sklearn's nice linear regression API\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % np.mean((regr.predict(x_test) - y_test) ** 2.0))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x_test, y_test))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(x_test, y_test, marker='o')\n",
    "plt.plot(x_test, regr.predict(x_test), color='blue', linewidth=1)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we had to *reshape* the x arrays in order to get them to work - that's because in general, x values aren't a single value, but instead an array of *features*, which are things that you look for. For example, a feature ($f_1$) could be the number of eyes in the picture, or the sum of all the pixel values, etc. Any measurable quantity can be a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than Linear Regression?\n",
    "Suppose we wanted to learn some nonlinear function of the data, like functions of the form \n",
    "\n",
    "$$y = a_1x^2 + a_2\\ln(x)$$\n",
    "\n",
    "(this is a crazy function I've never seen in practice). Linear regression actually generalizes really powerfully, because we can just *transform* the input to get the features we're looking for. For example, we can define the variables $r = x^2$ and $s = \\ln(x)$ - our function is then $y = a_1r + a_2s$, a linear function on $r$ and $s$. Let's try it in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# so we don't divide by 0!!!\n",
    "epsilon = 0.0000001\n",
    "\n",
    "x_train_transformed = np.array([[x**2, np.log(x + epsilon)] for x in x_train]).reshape(85, 2)\n",
    "x_test_transformed = np.array([[x**2, np.log(x + epsilon)] for x in x_test]).reshape(15, 2)\n",
    "\n",
    "# let's use sklearn's nice linear regression API\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# train the model using the training sets\n",
    "regr2.fit(x_train_transformed, y_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr2.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % np.mean((regr2.predict(x_test_transformed) - y_test) ** 2.0))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr2.score(x_test_transformed, y_test))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(x_test, y_test, marker='o')\n",
    "plt.plot(sorted(x_test), sorted(regr2.predict(x_test_transformed)), marker='o')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use linear regression?\n",
    "Use linear regression when you have the following:\n",
    "1. The `x` values are fixed, known values (or at least have very low variance compared to the residuals).\n",
    "2. Reasonable expectation that the data follows a linear model (this is usually the case, since by assumption (1) you can use as inputs any function of $x$ as a parameter to your regression).\n",
    "3. The residuals are normally distributed with constant variance, and are independent.\n",
    "\n",
    "Usually, the last assumption is the most difficult to satisfy, since if you have data across a large $x$-span, then the errors not usually univariate (think about what a 10% measurement error at 1mm looks like and what it looks like at 100m). Remember that *variance*, (or the square of standard deviation) is a scalar quantity, unlike z-score.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
